[{"content":"Why Do Simulations Matter? If you’ve ever tested a new idea in a sandbox before deploying it in the real world, you’ve used the same logic as a simulation study. In data science, simulation studies act like virtual laboratories for testing whether a proposed method works.\nBut what if this virtual lab doesn’t reflect real-world conditions? Simulations often rely on overly simplified, clean, and smooth data. These idealized setups may lead researchers to overestimate how well a method will work in the wild.\nThe Problem with Traditional Simulations Traditional Monte Carlo simulations often use handcrafted data-generating processes (DGPs) with neat bell-shaped distributions and simple rule-based logic. In reality, data is messy, nonlinear, and packed with complex dependencies.\nThis mismatch introduces a real risk: a method that performs well under artificial assumptions might fail in practice. (See our paper’s comparison of Figure 5 vs. Figure S6.)\nSimulation studies usually serve two purposes:\nPredicting how well a method will perform, Assessing how accurately it can recover true parameters. Our method focuses on the first: performance evaluation under realistic conditions.\nEnter Generative AI We propose a new simulation framework powered by Generative AI (GenAI) that learns from real-world data and generates realistic synthetic datasets for simulation studies.\nIf you\u0026rsquo;ve seen tools like ChatGPT or DALL·E in action, you already know how GenAI can create text or images based on learned patterns. We’re doing the same for data—but instead of text or art, we generate synthetic datasets that mirror real user behaviors (like how students interact with digital tests).\nWe use this synthetic data to evaluate a feature-extraction method proposed by Tang et al. (2020), comparing its performance on GenAI-based simulations versus traditional rule-based ones.\nReal Data, Real Behavior Our focus is on process data which are detailed records of how users interact with digital platforms. For example, when a student takes an online test, their interaction sequence might be:\nSTART → CLICK → CLICK → RETURN BUTTON → MARK BUTTON → CLICK → CHECKBOX_TAB → END\nWe use log data from the PIAAC international assessment and train two GenAI models:\nCTGAN (Xu et al., 2019): A GAN-based model for structured tabular data CPAR (Zhang et al., 2022): A neural network model for sequential categorical data The goal is to generate synthetic action sequences that closely mimic real behaviors.\nOur 5-Step GenAI Simulation Framework We designed a general-purpose simulation framework:\nPreprocess the data: Encode variables for model training. Train GenAI models: Learn realistic patterns from observed data. Assess data quality: Propose new evaluation metrics for sequential data: TVC-n: N-gram distribution similarity Recall-n / Precision-n: How well synthetic data captures true patterns Run simulations: Test statistical methods on GenAI-generated data. Evaluate method performance: Compare results to those from traditional simulations. What Did We Learn? GenAI-based simulations consistently generate more realistic synthetic data than traditional handcrafted DGPs. As a result, they offer more accurate insights into how statistical methods will perform on real-world data.\nThis approach helps bridge the gap between theory and practice—making simulations not only smarter, but more useful.\nReferences Xu, L., Skoularidou, M., Cuesta-Infante, A., \u0026amp; Veeramachaneni, K. (2019). Modeling Tabular Data using Conditional GAN. [Paper] Zhang, K., Patki, N., \u0026amp; Veeramachaneni, K. (2022). Sequential models in the synthetic data vault. [Paper] Tang, X., Wang, Z., He, Q., Liu, J., \u0026amp; Ying, Z. (2020). Latent feature extraction for process data via multidimensional scaling. Psychometrika, 85(2), 378-397.[Paper] Our Paper Suk, Y., Pan, C., \u0026amp; Yang, K. (2025). Using Generative AI for Sequential Data Generation in Monte Carlo Simulation Studies. [Preprint] ","permalink":"http://localhost:1313/post/genai_simulation_post/","summary":"This post introduces a generative AI framework that creates realistic synthetic data for simulation studies, enabling smarter performance evaluations of statistical methods.","title":"Making Simulations Smarter with Generative AI"},{"content":"Motivation\rCurrent educational systems often rely on one-size-fits-all plans, assuming that all students should follow the same learning path. However, students differ in their learning pace, interests, and long-term goals. While educators and school counselors may try to tailor recommendations based on student profiles, doing so effectively requires substantial training and deep familiarity with each individual’s background, which is not always feasible in real-world settings.\nTo address this challenge, we need data-driven approaches that can systematically leverage students’ learning histories to support more personalized and effective decision-making. Optimal Treatment Regime (OTR) methods from personalized medicine help to design individualized education plans. They recommend the most suitable learning path for each student based on their background and learning history and make personalized education more achievable.\nOptimal Treatment Regimes\rHow do OTR methods work in personalized education? Consider a treatment sets with multiple options (e.g., different courses). Different treatments (e.g., enrolling in Algebra I vs. Geometry) can lead to different potential outcomes for a given individual. OTRs aim to find the treatment that yields the best outcome for each person. Because treatment effects can vary across individuals, the optimal treatment may differ from one person to another. Instead of assigning the same treatment to everyone, OTRs leverage this treatment effect heterogeneity to recommend personalized treatment strategies.\nIn one sentence: OTRs aim to estimate the best treatment for each individual based on their observed characteristics, in order to maximize an outcome of interest.\nOTR for a Single Decision Point Set up\rTo start with, we consider OTRs for a single decision point, \\( K=1 \\). And we will extend to multiple decision points in future posts. For each individual, the data we observe are \\( \\{\\mathbf{X}, A, Y\\} \\), where\r- \\( \\mathbf{X}=\\{X_1, X_2, ..., X_p\\} \\) represents the baseline covariates (e.g., prior grades, motivation level),\r- \\(A \\in \\{a_1, a_2, a_3, ..., a_m\\} \\) is the treatment the individual receives from the treatment sets (e.g., course placement),\r- \\(Y\\) is the observed outcome given treatment \\(A\\) and covariates \\(X\\) (e.g., GPA, test score, or course completion). A treatment regime, or decision rule \\(d \\in \\mathcal{D}\\), assigns a treatment from the available treatment options to each individual based on their baseline covariates \\(\\mathbf{X}\\). The set \\(\\mathcal{D}\\) includes all possible single-decision treatment regimes and may consist of infinitely many decision rules \\(d\\).\nExamples of Treatment Regimes\rAssign all students to Algebra I regardless of their background. Assign students to Geometry if they took Algebra I in 8th grade; otherwise assign Algebra I. Assign students to Pre-algebra if their math GPAs in 8th grade are lower than 2.0; otherwise assign Algebra I. Since we can only observe the outcome under the treatment actually recieved, and not for all other treatment regimes, we need to define the potential outcome under regime \\(d\\). (The potential outcome is the outcome an individual would receive if assigned treatment according to regime \\(d\\).) The definition is:\r\\[\rY(d) = Y^a \\cdot \\mathbb{1}\\{d(X) = a\\}, \\]\rwhere \\(Y^a\\) is the potential outcome under treatment \\(a\\).\rPotential Outcome Framework\rThe potential outcomes define the outcomes an individual would experiences under each possible treatment. In the binary treatment case:\r\\( Y^1 \\): potential outcome if the individual receives treatment \\( Y^0 \\): potential outcome if the individual receives control Only one of these is observed for each individual, depending on the treatment actually received. The causal effect for an individual is defined as \\( Y^1 - Y^0 \\).\nThen, the optimal decision rule, \\(d^{opt}\\), is the decision rule that maximizes the expected potential outcome \\(Y(d)\\):\r\\[\rd^{\\text{opt}} = \\underset{d \\in \\mathcal{D}}{\\arg\\max} \\, \\mathbb{E}\\{ Y(d) \\}.\r\\]\rThe expected potential outcome of regime \\(d\\) is also called the value of regime \\(d\\). In short, an optimal treatment regime maximize the value.\rIdentification\rThe challenge here is that the potential outcome \\(Y(d)\\) in the formula above is not observable. To estimate \\(d^{opt}\\), we need to link the unobserved \\(Y(d)\\) to something we can observe. Basically, identification is the process to represent the unobserved causal quantity to some statistical quantity we can estimate under some assumptions. To identify OTR, we need three foundamental assumptions in causal inference:\nConsistency\rThe observed outcome \\(Y\\) equals the potential outcome under the treatment actually received.\rUnconfoundedness\rGiven covariates \\(\\mathbf{X}\\), the treatment assignment is independent of the potential outcomes.\rPositivity\rEach individual has a non-zero probability of receiving each treatment, given their covariates.\rWith the assumptions above, for binary treatment case, where \\(d(\\mathbf{X}) \\in \\{0,1\\}\\), the expected potential outcome (i.e., value) can be identified as:\n\\[\r\\mathbb{E}\\{Y(d)\\} = \\mathbb{E}[d(X)\\mathbb{E}[Y \\mid A = 1, X] + \\{1 - d(X)\\}\\mathbb{E}[Y \\mid A = 0, X]].\r\\]\rHow to derive this\r\\[\r\\begin{aligned}\r\\mathbb{E}\\{Y(d)\\} \u0026= \\mathbb{E}[d(X)Y^1 + \\{1 - d(X)\\}Y^0] \\\\\r\u0026= \\mathbb{E}[\\mathbb{E}[d(X)Y^1 + \\{1 - d(X)\\}Y^0 \\mid X]] \\\\\r\u0026= \\mathbb{E}[d(X)\\mathbb{E}[Y^1 \\mid X] + \\{1 - d(X)\\}\\mathbb{E}[Y^0 \\mid X]] \\\\\r\u0026= \\mathbb{E}[d(X)\\mathbb{E}[Y^1 \\mid A = 1, X] + \\{1 - d(X)\\}\\mathbb{E}[Y^0 \\mid A = 0, X]] \\text{(positivity, unconfoundedness)}\\\\\r\u0026= \\mathbb{E}[d(X)\\mathbb{E}[Y \\mid A = 1, X] + \\{1 - d(X)\\}\\mathbb{E}[Y \\mid A = 0, X]] \\text{(consistency)}\r\\end{aligned}\r\\]\rNow, the unobserved potential outcome is represented as observed outcome \\(Y\\). Based on that, we can identify the \\(d^{opt}\\) as \\[\rd^{opt} = \\mathbb{1}(\\mathbb{E}[Y \\mid X = x, A = 1] - \\mathbb{E}[Y \\mid X = x, A = 0] \u003e 0)\r\\]\rHow to derive this\rLet \\( Q(x, a) = \\mathbb{E}[Y \\mid X = x, A = a] \\). Then, we can rewrite \\( d^{opt} \\) as:\r\\[\r\\begin{aligned}\rd^{opt} \u0026= \\arg\\max_d \\mathbb{E}\\{Y(d)\\} \\\\\r\u0026= \\arg\\max_d \\mathbb{E}[d(X)Q(X,1) + \\{1 - d(X)\\}Q(X,0)] \\\\\r\u0026= \\arg\\max_d \\mathbb{E}[d(X)\\{Q(X,1) - Q(X,0)\\}] \\\\\r\u0026= \\mathbb{1}(Q(X,1) - Q(X,0) \u003e 0)\r\\end{aligned}\r\\]\rFor binary case, the optimal treatment rule assigns treatment when the expected outcome under treatment is larger than that under control, conditional on covariates \\(\\mathbf{X}\\).\nEstimation\rQ-learning estimates OTRs by modeling the conditional expectation of the outcome given covariates \\(\\mathbf{X}\\) and treatment \\(A\\), known as the **Q-function**:\r\\[\rQ(x, a) = \\mathbb{E}[Y \\mid X = x, A = a] \\]Once the Q-function is estimated from the data, the optimal treatment rule can be derived by choosing the treatment that yields the higher expected outcome. A simple and easy to implement choice to learn Q-function is linear regression model:\n\\[\rQ(X, A; \\boldsymbol{\\beta}) = \\beta_0 + \\beta_1 X + \\beta_2 A + \\beta_3 XA.\r\\]Then, the estimated OTR and value are straightforward:\n\\[\r\\begin{aligned}\r\\hat{d}^{opt}_Q(X) \u0026= \\arg\\max_{A \\in \\{0, 1\\}} Q(X, A; \\hat{\\boldsymbol{\\beta}})\\\\\r\u0026= \\mathbb{1} \\left\\{ Q(X, 1; \\hat{\\boldsymbol{\\beta}}) \u003e Q(X, 0; \\hat{\\boldsymbol{\\beta}}) \\right\\}\\\\\r\u0026= \\mathbb{1}(\\hat{\\beta}_2 + \\hat{\\beta}_3 X \u003e 0),\r\\end{aligned}\r\\]\\[\r\\begin{aligned}\r\\hat{V}_Q(d^{opt}) \u0026= \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + (\\hat{\\beta}_2 + \\hat{\\beta}_3 X_i)\\hat{d}^{opt}_Q(X) \\right]\\\\\r\u0026= \\frac{1}{n} \\sum_{i=1}^{n} \\left[ \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i + (\\hat{\\beta}_2 + \\hat{\\beta}_3 X_i)\\mathbb{1}(\\hat{\\beta}_2 + \\hat{\\beta}_3 X_i \u003e 0) \\right].\r\\end{aligned}\r\\]However, linear model relies heavily on correct specification of the outcome model and mis-specification can lead to biased treatment rules. In practice, the Q-function is typically estimated using more flexible models, such as generalized linear models or machine learning methods (e.g., random forests, XGBoost, neural networks), depending on the complexity of the data. We can also use some more robust methods like A-learning or targeted maximum likelihood estimation (TMLE).\nImplementation in R Suppose we are interested in whether students should take math course (\\(A=1\\)) or not in 9th grade. We measure their baseline covariates \\(X_1\\), the 8th grade math score and \\(X_2\\), the math identity. And we want to recommend the option that maximize their academic achievements \\(Y\\). A very useful package in r is DynTxRegime. Below, we will show how to estimate OTRs using this package.\n# Install and load the package install.packages(\u0026#34;DynTxRegime\u0026#34;) library(DynTxRegime) # Simulate data set.seed(123) n \u0026lt;- 1000 X1 \u0026lt;- runif(n, 0, 4) X2 \u0026lt;- rnorm(n, 0, 4) A \u0026lt;- rbinom(n, 1, 0.5) # Binary treatment Y \u0026lt;- 2 + 1.5 * A + 0.5 * X1 + 0.3 * X2 + 0.2 * X2 * A + rnorm(n) data \u0026lt;- data.frame(X1 = X1, X2 = X2, A = A, Y = Y) # Specify the models Main \u0026lt;- buildModelObj(model = ~ (X1 + X2), solver.method = \u0026#39;lm\u0026#39;, predict.method = \u0026#39;predict.lm\u0026#39;) # model for main effects Cont \u0026lt;- buildModelObj(model = ~ X2, solver.method = \u0026#39;lm\u0026#39;, predict.method = \u0026#39;predict.lm\u0026#39;) # model for treatment effects # Fit Q-learning using a linear model q_model \u0026lt;- qLearn( moMain = Main, # model for main effects (covariates) moCont = Cont, # model for contrasts (treatment effect modifiers) data = data, response = data$Y, # outcome txName = \u0026#34;A\u0026#34;, # name of the treatment variable verbose = TRUE ) # Summarize the model summary(q_model) # Get the coefficients coef(q_model) # Estimated optimal treatment regime otr \u0026lt;- optTx(q_model) otr # Estimated value value \u0026lt;- estimator(q_model) value References\rUnderstood.org – Personalized Learning: What You Need to Know Tsiatis et al. (2019) – Dynamic Treatment Regimes: Statistical Methods for Precision Medicine Publications/Working Papers in Our Lab Suk, Y., Park, C., Pan, C., \u0026amp; Kim, K. (2024). Fair and robust estimation of heterogeneous treatment effects for optimal policies in multilevel studies. PsyArXiv. [Preprint] [R Code]\nSuk, Y., \u0026amp; Park, C. (2023). Designing optimal, data-driven policies from multisite randomized trials. Psychometrika, 88. 1171-1196. [Journal Article] [Preprint] [R Code]\n","permalink":"http://localhost:1313/post/dtr/","summary":"Brief summary of the post (optional)","title":"Developing Personalized Education using Optimal Treatment Regimes"},{"content":"Headings Content here. Newline: \\ Bold text.\nItalic text.\nMath Formula If you want to include a in-line formula, please use: \\( d_t \\).\nFormula block:\n\\[\rA_t = d_t(S_t)\r\\]Multiple lines:\n\\[\r\\begin{aligned}\rQ(S_t, A_t) \u0026= R_t + \\gamma \\max_{a'} Q(S_{t+1}, a') \\\\\rV(S_t) \u0026= \\mathbb{E}_{A_t} [Q(S_t, A_t)]\r\\end{aligned}\r\\]Figure Please put your figure under static/ folder. And the path is \u0026quot;../../\u0026quot; + \u0026quot;path under static folder\u0026quot;.\nYou can add a figure using Markdown’s ![Alt Text](path) syntax:\n![Example](../../research/DTR.jpg) Code blocks a \u0026lt;- 5 print(a) ","permalink":"http://localhost:1313/post/example/","summary":"Brief summary of the post (optional)","title":"Example"}]